{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of new features for polymerist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence similarity comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Generator, Iterable, Optional, Sequence, Type, TypeVar, TypeAlias, Union \n",
    "Shape : TypeAlias = tuple\n",
    "T = TypeVar('T')\n",
    "N = TypeVar('N')\n",
    "M = TypeVar('M')\n",
    "\n",
    "from dataclasses import dataclass, field, replace\n",
    "from enum import Enum, auto\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from polymerist.genutils.bits import int_to_bits\n",
    "\n",
    "# REPRESENTATION CLASSES\n",
    "class EditOperation(Enum): # TODO: reimplement as bitwise flags\n",
    "    '''For annotating distinct kinds of sequence edits and their associated index offsets'''\n",
    "    NULL         = 0 # NOTE: order of fields here intentional and CANNOT be modified!\n",
    "    INSERTION    = 1 # ... this is because the bits of each index are the row and column\n",
    "    DELETION     = 2 # ... offsets corresponding to that edit operation in a Wagner-Fischer matrix\n",
    "    SUBSTITUTION = 3\n",
    "\n",
    "    @property\n",
    "    def bits(self) -> tuple[int, int]:\n",
    "        '''Convert the integer value of the Enum field into its binary bits'''\n",
    "        return tuple(int_to_bits(self.value, num_bits=2, as_list=True))\n",
    "    offsets = bits # alias to make dependent code more readable\n",
    "\n",
    "@dataclass\n",
    "class EditInfo:\n",
    "    '''for bundling together information about a sequence edit step'''\n",
    "    edit_op  : EditOperation\n",
    "    indices  : tuple[int, int]\n",
    "    distance : int\n",
    "\n",
    "\n",
    "# WAGNER-FISCHER MATRIX OPERATION\n",
    "def compute_wf_matrix(seq1 : Sequence[T], seq2 : Sequence[T], int_type : Type=int) -> np.ndarray[int]:\n",
    "    '''Compute (N+1)x(M+1) matrix of Levenshtein distances between all partial prefices of a pair of sequences\n",
    "    where N and M are the lengths of the first and second sequence, respectively. Implements the Wagner-Fischer algorithm'''\n",
    "    n, m = len(seq1), len(seq2)\n",
    "    n_aug, m_aug = n + 1, m + 1\n",
    "\n",
    "    # initialize matrix with all zeros apart from first row and column,\n",
    "    wf_matrix = np.zeros((n_aug, m_aug), dtype=int_type)\n",
    "    wf_matrix[:, 0] = np.arange(n_aug, dtype=int_type) # index along first column is same as number of deletions to get to empty sequence - NOTE: element [0, 0] overlaps here\n",
    "    wf_matrix[0, :] = np.arange(m_aug, dtype=int_type) # index along first row is same as number of insertions to get from empty sequence - NOTE: element [0, 0] overlaps here\n",
    "\n",
    "    # populate matrix by iterating on distances between sub-sequence problems\n",
    "    for i, elem1 in enumerate(seq1, start=1):\n",
    "        for j, elem2 in enumerate(seq2, start=1):\n",
    "            wf_matrix[i, j] = 1 + np.min([\n",
    "                wf_matrix[i-1, j],  # deletion at end of second sequence (after augmentation to end of first sequence)\n",
    "                wf_matrix[i, j-1],  # insertion at end of second sequence\n",
    "                wf_matrix[i-1, j-1] - int(elem1 == elem2) # substition of last elements between sequences (if elements are equal, then substitution costs nothing)\n",
    "            ])\n",
    "     # TODO: implement support for transposition weighting (i.e. Damerau-Levenshtein distance)\n",
    "\n",
    "    return wf_matrix\n",
    "\n",
    "def traverse_wf_matrix(wf_matrix : np.ndarray[Shape[N, M], int], begin_idxs : tuple[int, int]=(0, 0), end_idxs : tuple[int, int]=(-1, -1)) -> Generator[list[EditInfo], None, None]:\n",
    "    '''Takes a Wagner-Fischer Levenshtein distance matrix and returns the indices of the minimal path through the matrix\n",
    "    from the origin (i.e. empty sequences) to the '''\n",
    "    assert(wf_matrix.ndim == 2)\n",
    "    n_aug, m_aug = wf_matrix.shape\n",
    "\n",
    "    if end_idxs == begin_idxs:\n",
    "        yield [ # need to wrap in list for consistent typing in recursive calls\n",
    "            EditInfo( # base case\n",
    "                edit_op=EditOperation.NULL,\n",
    "                indices=begin_idxs,\n",
    "                distance=0\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        i, j = end_idxs\n",
    "        i %= n_aug # ensure values are positive\n",
    "        j %= m_aug # ensure values are positive\n",
    "        curr_dist = wf_matrix[i, j]\n",
    "\n",
    "        prev_edits = []\n",
    "        for edit_op in EditOperation:\n",
    "            if edit_op == EditOperation.NULL:\n",
    "                continue # need to skip over to avoid RecursionError - TOSELF: would be nice to reimplement EditOperations as bitwise flags to streamline this\n",
    "\n",
    "            di, dj = edit_op.offsets # unpack index offsets\n",
    "            i_prev, j_prev = i - di, j - dj\n",
    "            prev_edits.append(\n",
    "                EditInfo(\n",
    "                    edit_op=edit_op,\n",
    "                    indices=(i_prev, j_prev),\n",
    "                    distance=wf_matrix[i_prev, j_prev]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        min_prev_dist = min(ei.distance for ei in prev_edits)\n",
    "        for edit_info in prev_edits:\n",
    "            ret_edit_info = replace(edit_info, indices=(i, j)) # create a copy of the current edit info which reports the current \n",
    "            if (edit_info.edit_op == EditOperation.SUBSTITUTION) and (curr_dist == min_prev_dist):\n",
    "                ret_edit_info.edit_op = EditOperation.NULL\n",
    "                \n",
    "            if edit_info.distance == min_prev_dist:\n",
    "                for edit_steps in traverse_wf_matrix(wf_matrix, begin_idxs=begin_idxs, end_idxs=edit_info.indices): # recursive tail call through all possible predecessors\n",
    "                    yield edit_steps + [ret_edit_info]\n",
    "\n",
    "def describe_edits(seq1 : Sequence[T], seq2 : Sequence[T], int_type : Type=int, indicator : str=' -> ', delimiter : str='\\n') -> Generator[str, None, None]:\n",
    "    '''Describes step-by-step the insertion, deletion, or substitution operations needed to transform one sequence into another'''\n",
    "    seqs = (seq1, seq2) # need to bundle for zipping later\n",
    "    wf_matrix = compute_wf_matrix(seq1, seq2, int_type=int_type)\n",
    "\n",
    "    for edits in traverse_wf_matrix(wf_matrix):\n",
    "        edit_descs : list[str] = []\n",
    "        for edit_info in edits:\n",
    "            edit_op = edit_info.edit_op\n",
    "            if edit_op == EditOperation.NULL:\n",
    "                continue\n",
    "\n",
    "            elem_edit_str = indicator.join(\n",
    "                str(seq[idx - 1] if to_show else None) # need to subtract 1 to get index of step previous to current edit\n",
    "                    for idx, to_show, seq in zip(edit_info.indices, edit_op.bits, seqs)\n",
    "            )\n",
    "\n",
    "            edit_descs.append(f'{edit_info.edit_op.name}: {elem_edit_str}')\n",
    "        yield delimiter.join(edit_descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unday'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'Sunday'\n",
    "\n",
    "i = 0\n",
    "a[:i] + a[i+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday\n",
      "unday\n",
      "nday\n",
      "day\n",
      "ay\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate(a):\n",
    "    print(a[i:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEQUENCE METRICS\n",
    "def hamming_distance(seq1 : Sequence[T], seq2 : Sequence[T]) -> int:\n",
    "    '''Compute the Hamming distance between a pair of sequences with elements of compatible type (sequences must have the same length)\n",
    "    Denotes the number of elements at the same positions in each sequence which are different'''\n",
    "    if len(seq1) != len(seq2):\n",
    "        raise ValueError('Cannot compute Hamming distance between sequences of different lengths')\n",
    "    \n",
    "    return sum(\n",
    "        int(elem1 != elem2) # NOTE: type conversion not strictly necessary here, but done for self-documentation\n",
    "            for elem1, elem2 in zip(seq1, seq2, strict=True)\n",
    "    )\n",
    "\n",
    "def jaccard_distance(seq1 : Sequence[T], seq2 : Sequence[T]) -> float:\n",
    "    '''Compute the Jaccard distance between a pair of sequences with elements of compatible type\n",
    "    Denotes the complement of the ratio of shared elements (intersection) to total elements (union)'''\n",
    "    set1, set2 = set(seq1), set(seq2)\n",
    "    size_intersect = len(set.intersection(set1, set2))\n",
    "    size_union     = len(set.union(set1, set2))\n",
    "    jaccard_coeff = size_intersect / size_union\n",
    "\n",
    "    return 1 - jaccard_coeff\n",
    "tanimoto_distance = jaccard_distance # TOSELF: debatable whether this alias is really accurate (literature suggests it may be context/field-dependent)\n",
    "\n",
    "def levenshtein_distance(seq1 : Sequence[T], seq2 : Sequence[T], int_type : Type=int) -> int:\n",
    "    '''Compute the Levenshtein (edit) distance between a pair of sequences with elements of compatible type\n",
    "    Denotes the minimal number of insertion, deletion, or substitution operations needed to transform either sequence into the other'''\n",
    "    return compute_wf_matrix(seq1, seq2, int_type=int_type)[-1, -1]\n",
    "levenshtein_dist = edit_distance = edit_dist = levenshtein_distance # aliases for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'Sunday'\n",
    "b = 'Saturday'\n",
    "levenshtein_dist(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_matrix = compute_wf_matrix(a, b)\n",
    "wf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edits in traverse_wf_matrix(wf_matrix):\n",
    "    \n",
    "    mask = np.full_like(wf_matrix, fill_value=99)\n",
    "    for edit in edits:\n",
    "        print(edit)\n",
    "        mask[*edit.indices] = wf_matrix[*edit.indices]\n",
    "    print(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for desc in describe_edits(a, b):\n",
    "    print(desc)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reimplementing bin choice enumeration with dynamic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from itertools import product as cartesian_product\n",
    "\n",
    "def bin_ids_forming_sequence(\n",
    "        sequence : Sequence[T], choice_bins : Sequence[Iterable[T]], draw_without_repeats : bool=True, unique_bins : bool=False,\n",
    "        _symbol_inventory : Optional[defaultdict[Counter]]=None\n",
    "    ) -> Generator[tuple[int, ...], None, None]:\n",
    "    '''\n",
    "    Takes an ordered sequence of N objects of a given type and an ordered of any number of bins, each containing an arbitary amount of unordered objects of the same type\n",
    "    Generates all possible N-tuples of bin indices which could produce the target sequence when drawing from those bins in the \n",
    "    \n",
    "    if draw_without_repeats=True, will respect the multiplicity of elements in each bin when drawing\n",
    "    (i.e. will never have a bin position appear for a given object more times that that object appears in the corresponding bin)\n",
    "\n",
    "    if unique_bins=True, will only allow each bin to be sampled from once, EVEN if that bin contains elements which may occur later in the sequence\n",
    "    '''\n",
    "    if _symbol_inventory is None:\n",
    "        symbol_inventory = defaultdict(Counter) # keys are objects of type T (\"symbols\"), values give multiplicities of symbols keyed by bin position\n",
    "        for i, choice_bin in enumerate(choice_bins):\n",
    "            for sym in choice_bin:\n",
    "                symbol_inventory[sym][i] += 1 # NOTE : implementation here requires that T be a hashable type\n",
    "    else:\n",
    "        symbol_inventory = _symbol_inventory\n",
    "\n",
    "    print(symbol_inventory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'collections.Counter'>, {'b': Counter({0: 2, 2: 1}), 'c': Counter({4: 3, 0: 1, 1: 1}), 'a': Counter({3: 2, 1: 1}), 'e': Counter({1: 1, 3: 1, 4: 1}), 'd': Counter({1: 1, 2: 1, 3: 1}), 'f': Counter({4: 1}), 'g': Counter({5: 1})})\n"
     ]
    }
   ],
   "source": [
    "choice_bins = ['bbc', 'aced', 'bd', 'daea', 'fccce', 'g']\n",
    "bin_ids_forming_sequence('abc', choice_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General development of OpenMM utils and interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openff.toolkit import Molecule, Topology, ForceField\n",
    "\n",
    "smi = 'Oc1ccc(cc1)C(c2ccc(O)cc2)(C)C'\n",
    "offmol = Molecule.from_smiles(smi)\n",
    "offmol.generate_conformers(n_conformers=1)\n",
    "offmol.assign_partial_charges('am1bccelf10')\n",
    "\n",
    "forcefield = ForceField('openff-2.0.0.offxml')\n",
    "inc = forcefield.create_interchange(offmol.to_topology(), charge_from_molecules=[offmol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polymerist.mdtools.openmmtools.parameters import ThermoParameters, IntegratorParameters\n",
    "from polymerist.mdtools.openmmtools.thermo import EnsembleFactory\n",
    "from openmm.unit import femtosecond\n",
    "\n",
    "thermo_params = ThermoParameters()\n",
    "ensfac = EnsembleFactory.subclass_registry['NVT'](thermo_params)\n",
    "integrator = ensfac.integrator(time_step=2*femtosecond)\n",
    "\n",
    "ommsim = inc.to_openmm_simulation(integrator=integrator, combine_nonbonded_forces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openmm.unit import kilocalorie_per_mole, joule, kilojoule_per_mole\n",
    "\n",
    "pot, kin = eval_openmm_energies_separated(ommsim.context)\n",
    "pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ommsys = ommsim.context.getSystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing monomer graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from rdkit import Chem\n",
    "\n",
    "import mbuild\n",
    "from mbuild.compound import Compound\n",
    "from mbuild.conversion import load, load_smiles, from_rdkit, to_smiles, to_pybel\n",
    "from mbuild.lib.recipes.polymer import Polymer\n",
    "\n",
    "comp = mbuild.Compound()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String/graph translation (SMILES-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polymerist.polymers.monographs import MonomerGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openff.interchange.drivers import gromacs\n",
    "from openff.interchange import Interchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = f'<tests[-2]>.<tests[-2]>'\n",
    "tests = [\n",
    "    '[a]<-1>[A](<2-3>[Bee]<2=5>[C]<5=2>[Bee]<3-6>[Bee])(<2-3>[Bee](<3-6>[Bee])<3->[a])<->[A]<2-2>[Bee]<3-6>[Bee]',\n",
    "    '[A]<1-2>[B]<6=5>[C]<#>[D]',\n",
    "]\n",
    "seq = [0]\n",
    "test = '.'.join(tests[i] for i in seq)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail = '[tail]<2-0>' * 10 + '[tail_end]'\n",
    "lipid = f'[A3](<4-3>[A2]{tail})(<4-3>[A2]{tail})<2-0>{tail}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = MonomerGraph.from_SMIDGE(test)\n",
    "G = MonomerGraph.from_SMIDGE(lipid)\n",
    "G.visualize(label_monomers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = G.to_SMIDGE(start_node_idxs=6)\n",
    "H = MonomerGraph.from_SMIDGE(rep)\n",
    "H.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Alphabet\" of monomer fragment chemical information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_uppercase \n",
    "from polymerist.polymers.monomers import MonomerGroup\n",
    "from polymerist.rdutils.bonding.portlib import get_ports\n",
    "from polymerist.rdutils.labeling.molwise import clear_atom_map_nums\n",
    "\n",
    "\n",
    "parent_monomers = {\n",
    "    'ethane-1,2-diol' : 'OCCO',\n",
    "    'furan-2,5-dicarboxylic acid' : 'O=C(O)c1ccc(C(=O)O)o1',\n",
    "}\n",
    "monomer_aliases = {\n",
    "    mononame : lett*3\n",
    "        for mononame, lett in zip(parent_monomers.keys(), ascii_uppercase)\n",
    "}\n",
    "\n",
    "monogrp = MonomerGroup.from_file('poly(ethane-1,2-diol-co-furan-2,5-dicarboxylic acid).json')\n",
    "moldict, monosmiles = {}, {}\n",
    "for mononame, rdmol in monogrp.iter_rdmols():\n",
    "    for i, port in enumerate(get_ports(rdmol)):\n",
    "        rdmol.GetAtomWithIdx(port.linker.GetIdx()).SetIsotope(i)\n",
    "\n",
    "    print(mononame)\n",
    "    display(rdmol)\n",
    "    moldict[   mononame] = rdmol\n",
    "    monosmiles[mononame] = Chem.MolToSmiles(clear_atom_map_nums(rdmol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining monomer information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, ClassVar\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "from polymerist.genutils.fileutils.jsonio.jsonify import make_jsonifiable, dataclass_serializer_factory\n",
    "from polymerist.genutils.fileutils.jsonio.serialize import JSONSerializable, TypeSerializer\n",
    "from polymerist.rdutils.bonding.portlib import get_num_linkers, get_num_ports\n",
    "from polymerist.polymers.monomers.specification import expanded_SMILES, compliant_mol_SMARTS\n",
    "\n",
    "\n",
    "@make_jsonifiable\n",
    "@dataclass\n",
    "class MonomerFragmentInfo:\n",
    "    '''Naming and in-line chemical encodings for a monomer unit within a polymer chain'''\n",
    "    name   : str\n",
    "    smiles : str\n",
    "    exp_smiles : Optional[str] = field(default=None, init=False, repr=False)\n",
    "    smarts     : Optional[str] = field(default=None)\n",
    "    category   : Optional[str] = field(default=None)\n",
    "\n",
    "    n_atoms       : int = field(init=False)\n",
    "    functionality : int = field(init=False)\n",
    "    contribution  : int = field(init=False)\n",
    "\n",
    "    FOO : ClassVar[str] = 'extra bits'\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.exp_smiles = expanded_SMILES(self.smiles, assign_map_nums=True)\n",
    "        if self.smarts is None:\n",
    "            self.smarts = compliant_mol_SMARTS(self.exp_smiles)\n",
    "\n",
    "        tempmol = self.rdmol\n",
    "        self.n_atoms = tempmol.GetNumAtoms()\n",
    "        self.functionality = get_num_ports(tempmol) # get_num_linkers(tempmol) \n",
    "        self.contribution = self.n_atoms - self.functionality\n",
    "\n",
    "    @property\n",
    "    def rdmol(self) -> Chem.Mol:\n",
    "        return Chem.MolFromSmiles(self.smiles, sanitize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polymerist.polymers.monomers import specification\n",
    "\n",
    "mono_infos = {}\n",
    "for mononame, smiles in monosmiles.items():\n",
    "    parent_mononame = mononame.split('_')[0]\n",
    "    parent_smiles = parent_monomers[parent_mononame]\n",
    "    parent_alias  = monomer_aliases[parent_mononame]\n",
    "\n",
    "    mono_info = MonomerFragmentInfo(\n",
    "        name=mononame,\n",
    "        smiles=smiles,\n",
    "        # smarts=specification.compliant_mol_SMARTS(smiles),\n",
    "        category=parent_smiles,\n",
    "    )\n",
    "    alias = parent_alias.lower() if (mono_info.functionality == 1) else parent_alias.upper()\n",
    "    mono_infos[alias] = mono_info\n",
    "mono_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining polymer composition class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, StrEnum, auto\n",
    "\n",
    "from polymerist.genutils.fileutils.jsonio.serialize import JSONSerializable, TypeSerializer, MultiTypeSerializer\n",
    "from polymerist.genutils.fileutils.jsonio.jsonify import make_jsonifiable, JSONifiable\n",
    "from polymerist.polymers.monographs import MonomerGraph, MonomerGraphSerializer\n",
    "from polymerist.rdutils.bonding.portlib import get_num_linkers, get_ports\n",
    "\n",
    "\n",
    "MONOMER_CHARS = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
    "class MonomerNeighborMismatch(Enum):\n",
    "    '''For annotating the various ways in which a piece of monomer information in a monomer alphabet does not match a monomer graph'''\n",
    "    NONE = 0\n",
    "    COUNT = auto()\n",
    "    BONDTYPE = auto()\n",
    "    NO_FLAVOR = auto()\n",
    "    DIFF_FLAVOR = auto()\n",
    "    NOT_NEIGHBORS = auto()\n",
    "\n",
    "\n",
    "@make_jsonifiable(type_serializer=MultiTypeSerializer(MonomerGraphSerializer, MonomerFragmentInfo.serializer))\n",
    "@dataclass\n",
    "class PolymerStructure:\n",
    "    '''Encodes a multi-scale structural representation of a polymer topology'''\n",
    "    mono_alphabet : dict[str, MonomerFragmentInfo] \n",
    "    monograph : MonomerGraph\n",
    "\n",
    "    single_char_mononames : dict[str, str] = field(default_factory=dict, init=False) \n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        '''Post-process init attributes'''\n",
    "        self.single_char_mononames = { # remapping from the assigned monomers names to single characters for mbuild compatibility\n",
    "            mononame : remap_char\n",
    "                for (mononame, remap_char) in zip(self.mono_alphabet.keys(), MONOMER_CHARS)\n",
    "        }\n",
    "\n",
    "        self.validate_monoinfo_is_compatible() # will raise targetted exceptions if incompatible\n",
    "        self.assign_monoinfo_to_monograph()\n",
    "\n",
    "    @property\n",
    "    def node_info_map(self) -> dict[int, MonomerFragmentInfo]:\n",
    "        '''Map from node indices to relevant monomer information'''\n",
    "        return {\n",
    "            node_id : self.mono_alphabet[alias]\n",
    "                for node_id, alias in nx.get_node_attributes(self.monograph, self.monograph.MONOMER_NAME_ATTR).items()\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def pdb_substructures(self) -> dict[str, list[str]]:\n",
    "        '''Substructure dict formatted for the OpenFF Topology.from_pdb RDKit wrapper hook'''\n",
    "        return {\n",
    "            monoinfo.name : [mono_info.smarts]\n",
    "                for monoinfo in self.mono_alphabet.values()\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def num_atoms(self) -> int:\n",
    "        '''Total number of atoms in the topology specified'''\n",
    "        return sum(nx.get_node_attributes(self.monograph, 'contribution').values())\n",
    "\n",
    "    # validation\n",
    "    def monoalpha_surjective_to_monograph(self) -> bool:\n",
    "        '''Check whether the monomer alphabet covers all monomer types defined in the Graph'''\n",
    "        return self.monograph.unique_monomer_names.issubset(set(self.mono_alphabet.keys()))\n",
    "\n",
    "    def monoalpha_neighbors_are_valid(self) -> tuple[bool, int, MonomerNeighborMismatch]:\n",
    "        '''Determine whether and why adjacent monomers in the monomer graph do (or don't) have compatible chemical info'''\n",
    "        for node_idx, neighbor_dict in self.monograph.adj.items():\n",
    "            # 1) check that the number of neighbors in the graph matches the number of intermonomer bonding sites given chemically\n",
    "            neighbor_dict = dict(neighbor_dict) # convert from networkx object to vanilla dict\n",
    "            degree = len(neighbor_dict) # self.monograph.degree[node_idx]\n",
    "            if (degree != self.node_info_map[node_idx].functionality):\n",
    "                return False, node_idx, MonomerNeighborMismatch.COUNT\n",
    "            \n",
    "            # 2) check that all reported neighbor nodes are actually adjacent in the graph\n",
    "            found_ports = set()\n",
    "            for i, flavor in self.monograph.get_flavor_dict(node_idx).items():\n",
    "                if i not in neighbor_dict:\n",
    "                    return False, node_idx, MonomerNeighborMismatch.NOT_NEIGHBORS\n",
    "                \n",
    "                nb_bond_info = neighbor_dict.pop(i)\n",
    "                found_ports.add( (flavor, nb_bond_info[self.monograph.BONDTYPE_ATTR]) )\n",
    "\n",
    "            # 3) check that every neighbors has been provided a flavor\n",
    "            if neighbor_dict:\n",
    "                return False, node_idx, MonomerNeighborMismatch.NO_FLAVOR # at least one of the neighbors must nnot have had a flavor provided\n",
    "\n",
    "            # 4) check that the provided flavors match those chemically specified\n",
    "            monoinfo = self.mono_alphabet[self.monograph.get_monomer_name(node_idx)]\n",
    "            portinfo = set(\n",
    "                (port.flavor, port.bond.GetBondType())\n",
    "                    for port in get_ports(monoinfo.rdmol)\n",
    "            )\n",
    "            if (portinfo != found_ports):\n",
    "                return False, node_idx, MonomerNeighborMismatch.DIFF_FLAVOR\n",
    "        else:\n",
    "            return True, -1, MonomerNeighborMismatch.NONE\n",
    "        \n",
    "    def validate_monoinfo_is_compatible(self) -> None:\n",
    "        if not self.monoalpha_surjective_to_monograph():\n",
    "            raise ValueError('Provided monomer alphabet does not cover all monomers in the corresponding monomer graph')\n",
    "        \n",
    "        nb_match, mismatch_idx, reason = self.monoalpha_neighbors_are_valid()\n",
    "        if not nb_match:\n",
    "            raise ValueError(f'Graph node {mismatch_idx} (designation \"{self.monograph.get_monomer_name(mismatch_idx)}\") mismatched (reason : {reason.name})')\n",
    "    \n",
    "    def assign_monoinfo_to_monograph(self) -> None:\n",
    "        '''Map the chemical info for each unique monomer onto corresponding monomer nodes in the monomer graph'''\n",
    "        node_info_map = {\n",
    "            node_idx : self.mono_alphabet[self.monograph.monomer_name(node_idx)].__dict__\n",
    "                for node_idx in self.monograph.nodes\n",
    "        }\n",
    "        nx.set_node_attributes(self.monograph, node_info_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from polymerist.genutils.textual import delimiters\n",
    "from polymerist.polymers.monographs import MonomerGraph\n",
    "\n",
    "smidge = 'aBABABABa'\n",
    "# smidge = '{2-3}'.join(smidge)\n",
    "# smidge = '{0-1}'.join(smidge)\n",
    "smidge = '<0-1>'.join(smidge[:-1]) + '<0-0>' + smidge[-1]\n",
    "smidge = delimiters.square_brackets_around_letters(smidge)\n",
    "smidge = ''.join(3*c if c.isalpha() else c for c in smidge)\n",
    "print(smidge)\n",
    "\n",
    "monograph = MonomerGraph.from_smidge(smidge)\n",
    "monograph.draw()\n",
    "\n",
    "poly = PolymerStructure(\n",
    "    mono_alphabet=mono_infos,\n",
    "    monograph=monograph\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing JSON I/O\n",
    "poly.to_file('test.json')\n",
    "poly2 = PolymerStructure.from_file('test.json')\n",
    "poly2.monograph.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbuild.lib.recipes import Polymer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing mbuild coordinate generator hook for linear polymer graphs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly.monograph.is_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in poly.monograph.termini:\n",
    "    print(poly.monograph.nodes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polymerist.genutils.textual.strsearch import shortest_repeating_substring\n",
    "\n",
    "\n",
    "terms = list(monograph.termini)\n",
    "assert(len(terms) == 2)\n",
    "head_node = terms[0]\n",
    "\n",
    "seq = ''\n",
    "for i in nx.dfs_preorder_nodes(monograph, source=head_node):\n",
    "    if i not in terms:\n",
    "        mononame = monograph.nodes[i][monograph.MONOMER_NAME_ATTR]\n",
    "        seq += poly.single_char_mononames[mononame]\n",
    "min_seq = shortest_repeating_substring(seq)\n",
    "seq, min_seq, seq.count(min_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polymerist.rdutils.bonding.portlib import Port\n",
    "from polymerist.rdutils.bonding.substitution import saturate_ports\n",
    "\n",
    "Port.bondable_flavors.reset()\n",
    "Port.bondable_flavors.insert((0,2))\n",
    "Port.bondable_flavors.insert((1,2))\n",
    "\n",
    "rm = mono_info.rdmol\n",
    "newmol = saturate_ports(rm, cap=Chem.MolFromSmiles('*-[2H]', sanitize=False), flavor_to_saturate=0)\n",
    "display(newmol)\n",
    "for atom in newmol.GetAtoms():\n",
    "    print(atom.GetIdx(), atom.GetSymbol(), atom.GetIsotope())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polymerist.rdutils.bonding.substitution import hydrogenate_rdmol_ports\n",
    "from mbuild.conversion import from_rdkit\n",
    "from polymerist.polymers.building import mbmol_to_openmm_pdb\n",
    "\n",
    "\n",
    "prot_mol = hydrogenate_rdmol_ports(mono_info.rdmol)\n",
    "Chem.SanitizeMol(prot_mol, sanitizeOps=specification.SANITIZE_AS_KEKULE)\n",
    "mbmol = from_rdkit(prot_mol)\n",
    "mbmol_to_openmm_pdb('test.pdb', mbmol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with rich progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.progress import track, Progress\n",
    "from time import sleep\n",
    "\n",
    "with Progress() as progress:\n",
    "\n",
    "    task1 = progress.add_task(\"[red]Downloading...\", total=1000)\n",
    "    task2 = progress.add_task(\"[green]Processing...\", total=1000)\n",
    "    task3 = progress.add_task(\"[cyan]Cooking...\", total=1000)\n",
    "\n",
    "    while not progress.finished:\n",
    "        progress.update(task1, advance=0.5)\n",
    "        progress.update(task2, advance=0.3)\n",
    "        progress.update(task3, advance=0.9)\n",
    "        sleep(0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from rich.progress import (\n",
    "    BarColumn,\n",
    "    Progress,\n",
    "    SpinnerColumn,\n",
    "    TaskProgressColumn,\n",
    "    TimeElapsedColumn,\n",
    "    TimeRemainingColumn,\n",
    ")\n",
    "\n",
    "def process(chunks):\n",
    "    for chunk in chunks:\n",
    "        time.sleep(0.1)\n",
    "        yield chunk\n",
    "\n",
    "chunks = [random.randint(1,20) for _ in range(100)]\n",
    "\n",
    "progress_columns = (\n",
    "    SpinnerColumn(),\n",
    "    \"[progress.description]{task.description}\",\n",
    "    BarColumn(),\n",
    "    TaskProgressColumn(),\n",
    "    \"Elapsed:\",\n",
    "    TimeElapsedColumn(),\n",
    "    \"Remaining:\",\n",
    "    TimeRemainingColumn(),\n",
    ")\n",
    "\n",
    "with Progress(*progress_columns) as progress_bar:\n",
    "    task = progress_bar.add_task(\"[blue]Downloading...\", total=sum(chunks))\n",
    "    for chunk in process(chunks):\n",
    "        progress_bar.update(task, advance=chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "from rich.live import Live\n",
    "from rich.table import Table\n",
    "\n",
    "\n",
    "def generate_table() -> Table:\n",
    "    \"\"\"Make a new table.\"\"\"\n",
    "    table = Table()\n",
    "    table.add_column(\"ID\")\n",
    "    table.add_column(\"Value\")\n",
    "    table.add_column(\"Status\")\n",
    "\n",
    "    for row in range(random.randint(2, 6)):\n",
    "        value = random.random() * 100\n",
    "        table.add_row(\n",
    "            f\"{row}\", f\"{value:3.2f}\", \"[red]ERROR\" if value < 50 else \"[green]SUCCESS\"\n",
    "        )\n",
    "    return table\n",
    "\n",
    "\n",
    "with Live(generate_table(), refresh_per_second=4) as live:\n",
    "    for _ in range(40):\n",
    "        time.sleep(0.4)\n",
    "        live.update(generate_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from rich.console import Console, ConsoleOptions, RenderResult\n",
    "from rich.table import Table\n",
    "\n",
    "@dataclass\n",
    "class Student:\n",
    "    id: int\n",
    "    name: str\n",
    "    age: int\n",
    "    def __rich_console__(self, console: Console, options: ConsoleOptions) -> RenderResult:\n",
    "        yield f\"[b]Student:[/b] #{self.id}\"\n",
    "        my_table = Table(\"Attribute\", \"Value\")\n",
    "        my_table.add_row(\"name\", self.name)\n",
    "        my_table.add_row(\"age\", str(self.age))\n",
    "        yield my_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polymerist-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
