{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of new features for polymerist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reimplementing bin choice enumeration with dynamic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator, Iterable, Sequence, TypeVar, TypeAlias\n",
    "Shape : TypeAlias = tuple\n",
    "T = TypeVar('T')\n",
    "N = TypeVar('N')\n",
    "M = TypeVar('M')\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import product as cartesian_product\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_symbol_inventory(choice_bins : Sequence[Iterable[T]]) -> dict[T, Counter[int, int]]:\n",
    "    '''\n",
    "    Accepts an ordered collection of bins of elements of type T (\"symbols\")\n",
    "\n",
    "    Returns a dict, keyed by symbol, whose values count the number of occurrences\n",
    "    of that symbol in all bins in which that symbol occurs\n",
    "    ''' \n",
    "    symbol_inventory = defaultdict(Counter) # keys are objects of type T (\"symbols\"), values give multiplicities of symbols keyed by bin position\n",
    "    for i, choice_bin in enumerate(choice_bins):\n",
    "        for sym in choice_bin:\n",
    "            symbol_inventory[sym][i] += 1 # NOTE : implementation here requires that T be a hashable type\n",
    "    \n",
    "    return dict(symbol_inventory)\n",
    "\n",
    "def create_occurence_matrix(choice_bins : Sequence[Iterable[T]]) -> tuple[np.ndarray[Shape[N, M], int], dict[T, int]]:\n",
    "    '''Creates an occurence matrix from a sequence of M unordered bins containing N distinct elements of type T (\"symbols\")\n",
    "    Matrix element A_ij denotes the number of occurences of the i-th symbol (according to an arbitrary numbering) in the j-th bin\n",
    "\n",
    "    Returns the occurence matrix as an array, along with a dict mapping each symbol to a unique index'''\n",
    "    symbol_inventory = create_symbol_inventory(choice_bins)\n",
    "    symbol_indices = {} # for keeping track of the index each symbol is mapped to\n",
    "\n",
    "    shape = n_symbols, n_bins = len(symbol_inventory), len(choice_bins)\n",
    "    occurence_matrix = np.zeros(shape, dtype=int)\n",
    "\n",
    "    for i, (symbol, counter) in enumerate(symbol_inventory.items()):\n",
    "        symbol_indices[symbol] = i\n",
    "        for j, num_occurences in counter.items():\n",
    "            occurence_matrix[i, j] = num_occurences\n",
    "    \n",
    "    return occurence_matrix, symbol_indices\n",
    "\n",
    "\n",
    "def _bin_ids_forming_sequence_recursive(\n",
    "        sequence : Sequence[T],\n",
    "        symbol_inventory : dict[T, Counter[int, int]],\n",
    "        ignore_multiplicities : bool=False,\n",
    "        unique_bins : bool=False,\n",
    "        _buffer : tuple[int]=None,\n",
    "    ) -> Generator[tuple[int, ...], None, None]:\n",
    "    '''\n",
    "    Takes an ordered sequence of N objects of a given type (\"symbols\") and an ordered sequence of \"bins\" of items of the same type\n",
    "    Generates all possible N-tuples of bin indices which could produce the target sequence when drawn from in that order\n",
    "\n",
    "    If ignore_multiplicities=True, will not respect the counts of elements in each bin when drawing\n",
    "    (i.e. for any given symbol, would allow a bin containing that symbol to appear more times than that symbol is present)\n",
    "\n",
    "    If unique_bins=True, will only allow each bin to be sampled from once, EVEN if that bin contains elements which may occur later in the sequence\n",
    "    \n",
    "    Disclaimers:\n",
    "    - Order of symbols in each bin is irrelevant, only the multiplicities of each unique symbol matter (and even then, only if ignore_multiplicities=True)\n",
    "    - For implementation reasons, it is required that the type T be hashable\n",
    "    '''\n",
    "    if _buffer is None:\n",
    "        _buffer = tuple()\n",
    "\n",
    "    # base case for recursion\n",
    "    if not sequence:\n",
    "        yield _buffer # yields empty buffer if no sequence is present\n",
    "        return\n",
    "\n",
    "    # implicit else\n",
    "    symbol_cost = (0 if ignore_multiplicities else 1) # NOTE: definition here could be made more terse, but at the expense of readability\n",
    "    symbol, *sequence_copy = sequence # separate head symbol from rest of sequence\n",
    "    \n",
    "    for bin_idx, num_occurences in symbol_inventory[symbol].items():\n",
    "        # only proceed if letters are available from that bin, AND either uniqueness is not required, or it is required but has not yet been violated\n",
    "        if (num_occurences > 0) and (not unique_bins or (bin_idx not in _buffer)):\n",
    "            symbol_inventory[symbol][bin_idx] -= symbol_cost # mark current symbol and bin in symbol inventory and visit tracker, respectively\n",
    "\n",
    "            yield from _bin_ids_forming_sequence_recursive( # recursive traversal of remainder of sequence with current choice of bin for leading character\n",
    "                sequence=sequence_copy,\n",
    "                symbol_inventory=symbol_inventory,\n",
    "                ignore_multiplicities=ignore_multiplicities,\n",
    "                unique_bins=unique_bins,\n",
    "                _buffer=_buffer + (bin_idx,), # creates copy, rather than modifying the buffer for the current symbol (i.e. exactly what we want)\n",
    "            )\n",
    "            symbol_inventory[symbol][bin_idx] += symbol_cost # replace removed symbol for subsequent traversals to avoid polluting other states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bin_ids_forming_sequence_stack(\n",
    "        sequence : Sequence[T],\n",
    "        symbol_inventory : dict[T, Counter[int, int]],\n",
    "        ignore_multiplicities : bool=False,\n",
    "        unique_bins : bool=False,\n",
    "    ) -> Generator[tuple[int, ...], None, None]:\n",
    "    '''insert docs here'''\n",
    "    symbol_cost = (0 if ignore_multiplicities else 1) # NOTE: definition here could be made more terse, but at the expense of readability\n",
    "\n",
    "    N = len(sequence)\n",
    "    if not sequence:\n",
    "        yield tuple()\n",
    "        return\n",
    "\n",
    "    buffer = []\n",
    "    def valid_bins(sym : T) -> list[T]:\n",
    "        return [\n",
    "            bin_id\n",
    "                for bin_id, count in symbol_inventory[sym].items()\n",
    "                    if (count > 0) and (not unique_bins or (bin_id not in buffer))\n",
    "        ]\n",
    "\n",
    "    bin_stack = [\n",
    "        valid_bins(sequence[0]) if sequence else []\n",
    "    ]\n",
    "    \n",
    "    steps : int = 0\n",
    "    while bin_stack:\n",
    "        if len(buffer) == N:\n",
    "            yield tuple(buffer)\n",
    "            bin_stack.append([]) # calls backtrack on next iteration, returning symbol while still pointing to the same position in the word\n",
    "\n",
    "        bins_to_check = bin_stack.pop()\n",
    "        if not bins_to_check:\n",
    "            if buffer:\n",
    "                latest_bin_id = buffer.pop()\n",
    "                symbol = sequence[len(buffer)] # NOTE: symbol update fetches need to be done carefully WRT buffer updates\n",
    "                # print(symbol, 'empty buf', steps)\n",
    "                # print(len(buffer), len(bin_stack))\n",
    "                symbol_inventory[symbol][latest_bin_id] += symbol_cost\n",
    "        else:\n",
    "            next_bin_id = bins_to_check.pop(0) # NOTE: don't strictly need to pop FIRST item, but doing so matches output order of recursive implementation\n",
    "            bin_stack.append(bins_to_check)    # return remaining list of bins to check to stack for further traversal\n",
    "            \n",
    "            symbol = sequence[len(buffer)] # NOTE: symbol update fetches need to be done carefully WRT buffer updates\n",
    "            # print(symbol, 'pushed to buf', steps)\n",
    "            # print(len(buffer), len(bin_stack))\n",
    "            symbol_inventory[symbol][next_bin_id] -= symbol_cost\n",
    "            buffer.append(next_bin_id)\n",
    "            \n",
    "            if len(buffer) != N:\n",
    "                symbol = sequence[len(buffer)] # NOTE: symbol update fetches need to be done carefully WRT buffer updates\n",
    "                # print(symbol, 'pushed to stack', steps)\n",
    "                # print(len(buffer), len(bin_stack))\n",
    "                bin_stack.append(valid_bins(symbol))\n",
    "        \n",
    "        steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bin_ids_forming_sequence_stack_rev(\n",
    "        sequence : Sequence[T],\n",
    "        symbol_inventory : dict[T, Counter[int, int]],\n",
    "        ignore_multiplicities : bool=False,\n",
    "        unique_bins : bool=False,\n",
    "    ) -> Generator[tuple[int, ...], None, None]:\n",
    "    '''insert docs here'''\n",
    "    symbol_cost = (0 if ignore_multiplicities else 1) # NOTE: definition here could be made more terse, but at the expense of readability\n",
    "\n",
    "    N = len(sequence)\n",
    "    # if not sequence:\n",
    "    #     yield tuple()\n",
    "    #     return\n",
    "\n",
    "    mixed_stack : list[tuple[T, list[int]]] = [(s, []) for s in sequence] if sequence else [(None, [])]\n",
    "    buffer = []\n",
    "    # bin_stack = [[]]\n",
    "    #     # valid_bins(sequence[0]) if sequence else []\n",
    "    # # ]\n",
    "    # symbol_stack = [s for s in sequence] # reverse to allow consistency viz pop() and append()\n",
    "    # print('init', buffer, symbol_stack)\n",
    "\n",
    "    while mixed_stack:\n",
    "        symbol, bin_substack = mixed_stack.pop(0)\n",
    "    # while bin_stack or symbol_stack:\n",
    "        # symbol = symbol_stack.pop(0) if symbol_stack else None # null case handles when symbols are exhausted\n",
    "        print('start: ', symbol, bin_substack, buffer)\n",
    "        if len(buffer) == N: # yield solution after completed traversal of sequence\n",
    "            yield tuple(buffer)\n",
    "            # mixed_stack.append( (symbol, []) ) # empty bin substack signifies no successors\n",
    "        else: # replenish at intermediate steps\n",
    "            next_bins = [ # will remain empty if no successors remain to be drawn from the inventory\n",
    "                bin_id\n",
    "                    for bin_id, count in symbol_inventory[symbol].items()\n",
    "                        if (count > 0) and (not unique_bins or (bin_id not in buffer))\n",
    "            ] \n",
    "            mixed_stack.append( (symbol, next_bins) )\n",
    "        \n",
    "        if symbol:\n",
    "            if not bin_substack: # if no successors exist, roll back buffer and return current symbol to inverntory\n",
    "                if buffer: # both must be non-null for this to be a valid operation\n",
    "                    latest_bin_id = buffer.pop()\n",
    "                    symbol_inventory[symbol][latest_bin_id] += symbol_cost\n",
    "            else:\n",
    "                next_bin_id = bin_substack.pop(0) # NOTE: don't strictly need to pop FIRST item, but doing so matches output order of recursive implementation\n",
    "                mixed_stack.append( (symbol, bin_substack) )    # return remaining list of bins to check to stack for further traversal\n",
    "                symbol_inventory[symbol][next_bin_id] -= symbol_cost # withdraw symbol from inventory...\n",
    "                buffer.append(next_bin_id) # ...and push the bin it occurs in to the buffer\n",
    "        print('end: ', symbol, mixed_stack, buffer)\n",
    "        print('='*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_multiplicities=False\n",
    "unique_bins=False\n",
    "\n",
    "choice_bins = ('bbc', 'aced', 'bad')#, 'daea', 'fccce', 'g')\n",
    "word = 'abc'\n",
    "sym_inv = create_symbol_inventory(choice_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = set()\n",
    "for idxs in _bin_ids_forming_sequence_stack_rev(word, sym_inv, ignore_multiplicities=ignore_multiplicities, unique_bins=unique_bins):\n",
    "    print(idxs)\n",
    "    res3.add(idxs)\n",
    "print('='*50)\n",
    "print(res3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = set()\n",
    "for idxs in _bin_ids_forming_sequence_stack(word, sym_inv, ignore_multiplicities=ignore_multiplicities, unique_bins=unique_bins):\n",
    "    print(idxs)\n",
    "    res1.add(idxs)\n",
    "print('='*50)\n",
    "\n",
    "res2 = set()\n",
    "for idxs in _bin_ids_forming_sequence_recursive(word, sym_inv, ignore_multiplicities=ignore_multiplicities, unique_bins=unique_bins):\n",
    "    print(idxs)\n",
    "    res2.add(idxs)\n",
    "print('='*50)\n",
    "\n",
    "print(res1 - res2, res2 - res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_wrap_str(string : str) -> Generator[str, None, None]:\n",
    "    for char in string:\n",
    "        yield char\n",
    "\n",
    "\n",
    "choice_bins = ('bbc', 'aced', 'bd', 'daea', 'fccce', 'g')\n",
    "choice_bins_gen = [\n",
    "    gen_wrap_str(string)\n",
    "        for string in choice_bins\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openff import interchange\n",
    "from polymerist.genutils.importutils import module_hierarchy\n",
    "print(module_hierarchy(interchange))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General development of OpenMM utils and interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openff.toolkit import Molecule, Topology, ForceField\n",
    "\n",
    "smi = 'Oc1ccc(cc1)C(c2ccc(O)cc2)(C)C'\n",
    "offmol = Molecule.from_smiles(smi)\n",
    "offmol.generate_conformers(n_conformers=1)\n",
    "offmol.assign_partial_charges('am1bccelf10')\n",
    "\n",
    "forcefield = ForceField('openff-2.0.0.offxml')\n",
    "inc = forcefield.create_interchange(offmol.to_topology(), charge_from_molecules=[offmol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polymerist.mdtools.openmmtools.parameters import ThermoParameters, IntegratorParameters\n",
    "from polymerist.mdtools.openmmtools.thermo import EnsembleFactory\n",
    "from openmm.unit import femtosecond\n",
    "\n",
    "thermo_params = ThermoParameters()\n",
    "ensfac = EnsembleFactory.subclass_registry['NVT'](thermo_params)\n",
    "integrator = ensfac.integrator(time_step=2*femtosecond)\n",
    "\n",
    "ommsim = inc.to_openmm_simulation(integrator=integrator, combine_nonbonded_forces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openmm.unit import kilocalorie_per_mole, joule, kilojoule_per_mole\n",
    "\n",
    "pot, kin = eval_openmm_energies_separated(ommsim.context)\n",
    "pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ommsys = ommsim.context.getSystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing monomer graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from rdkit import Chem\n",
    "\n",
    "import mbuild\n",
    "from mbuild.compound import Compound\n",
    "from mbuild.conversion import load, load_smiles, from_rdkit, to_smiles, to_pybel\n",
    "from mbuild.lib.recipes.polymer import Polymer\n",
    "\n",
    "comp = mbuild.Compound()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String/graph translation (SMILES-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polymerist.polymers.monographs import MonomerGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openff.interchange.drivers import gromacs\n",
    "from openff.interchange import Interchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = f'<tests[-2]>.<tests[-2]>'\n",
    "tests = [\n",
    "    '[a]<-1>[A](<2-3>[Bee]<2=5>[C]<5=2>[Bee]<3-6>[Bee])(<2-3>[Bee](<3-6>[Bee])<3->[a])<->[A]<2-2>[Bee]<3-6>[Bee]',\n",
    "    '[A]<1-2>[B]<6=5>[C]<#>[D]',\n",
    "]\n",
    "seq = [0]\n",
    "test = '.'.join(tests[i] for i in seq)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail = '[tail]<2-0>' * 10 + '[tail_end]'\n",
    "lipid = f'[A3](<4-3>[A2]{tail})(<4-3>[A2]{tail})<2-0>{tail}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = '<->[A3](<=>[B0])<=>[B0]'\n",
    "b2 = '<->[B2]<->[A3](<=>[B0])<->[B2]'\n",
    "b3 = '[A3]<->[B(<->[B2])<->[B2]'\n",
    "branched = f'[A3]({b2}{b1})({b2}{b2}{b1})({b2}({b1}){b2}{b1})'\n",
    "branched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targ_smidge = test\n",
    "# targ_smidge = lipid\n",
    "targ_smidge = branched\n",
    "\n",
    "G = MonomerGraph.from_SMIDGE(targ_smidge)\n",
    "G.visualize(label_monomers=True, label_bonds=True, font_size=10, font_color='yellow', node_size=300, pos=nx.kamada_kawai_layout(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{i : 5*v for i, v in nx.spring_layout(G).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = G.to_SMIDGE(start_node_idxs=6)\n",
    "H = MonomerGraph.from_SMIDGE(rep)\n",
    "H.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Alphabet\" of monomer fragment chemical information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_uppercase \n",
    "from polymerist.polymers.monomers import MonomerGroup\n",
    "from polymerist.rdutils.bonding.portlib import get_ports\n",
    "from polymerist.rdutils.labeling.molwise import clear_atom_map_nums\n",
    "\n",
    "\n",
    "parent_monomers = {\n",
    "    'ethane-1,2-diol' : 'OCCO',\n",
    "    'furan-2,5-dicarboxylic acid' : 'O=C(O)c1ccc(C(=O)O)o1',\n",
    "}\n",
    "monomer_aliases = {\n",
    "    mononame : lett*3\n",
    "        for mononame, lett in zip(parent_monomers.keys(), ascii_uppercase)\n",
    "}\n",
    "\n",
    "monogrp = MonomerGroup.from_file('poly(ethane-1,2-diol-co-furan-2,5-dicarboxylic acid).json')\n",
    "moldict, monosmiles = {}, {}\n",
    "for mononame, rdmol in monogrp.iter_rdmols():\n",
    "    for i, port in enumerate(get_ports(rdmol)):\n",
    "        rdmol.GetAtomWithIdx(port.linker.GetIdx()).SetIsotope(i)\n",
    "\n",
    "    print(mononame)\n",
    "    display(rdmol)\n",
    "    moldict[   mononame] = rdmol\n",
    "    monosmiles[mononame] = Chem.MolToSmiles(clear_atom_map_nums(rdmol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining monomer information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, ClassVar\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "from polymerist.genutils.fileutils.jsonio.jsonify import make_jsonifiable, dataclass_serializer_factory\n",
    "from polymerist.genutils.fileutils.jsonio.serialize import JSONSerializable, TypeSerializer\n",
    "from polymerist.rdutils.bonding.portlib import get_num_linkers, get_num_ports\n",
    "from polymerist.polymers.monomers.specification import expanded_SMILES, compliant_mol_SMARTS\n",
    "\n",
    "\n",
    "@make_jsonifiable\n",
    "@dataclass\n",
    "class MonomerFragmentInfo:\n",
    "    '''Naming and in-line chemical encodings for a monomer unit within a polymer chain'''\n",
    "    name   : str\n",
    "    smiles : str\n",
    "    exp_smiles : Optional[str] = field(default=None, init=False, repr=False)\n",
    "    smarts     : Optional[str] = field(default=None)\n",
    "    category   : Optional[str] = field(default=None)\n",
    "\n",
    "    n_atoms       : int = field(init=False)\n",
    "    functionality : int = field(init=False)\n",
    "    contribution  : int = field(init=False)\n",
    "\n",
    "    FOO : ClassVar[str] = 'extra bits'\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.exp_smiles = expanded_SMILES(self.smiles, assign_map_nums=True)\n",
    "        if self.smarts is None:\n",
    "            self.smarts = compliant_mol_SMARTS(self.exp_smiles)\n",
    "\n",
    "        tempmol = self.rdmol\n",
    "        self.n_atoms = tempmol.GetNumAtoms()\n",
    "        self.functionality = get_num_ports(tempmol) # get_num_linkers(tempmol) \n",
    "        self.contribution = self.n_atoms - self.functionality\n",
    "\n",
    "    @property\n",
    "    def rdmol(self) -> Chem.Mol:\n",
    "        return Chem.MolFromSmiles(self.smiles, sanitize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polymerist.polymers.monomers import specification\n",
    "\n",
    "mono_infos = {}\n",
    "for mononame, smiles in monosmiles.items():\n",
    "    parent_mononame = mononame.split('_')[0]\n",
    "    parent_smiles = parent_monomers[parent_mononame]\n",
    "    parent_alias  = monomer_aliases[parent_mononame]\n",
    "\n",
    "    mono_info = MonomerFragmentInfo(\n",
    "        name=mononame,\n",
    "        smiles=smiles,\n",
    "        # smarts=specification.compliant_mol_SMARTS(smiles),\n",
    "        category=parent_smiles,\n",
    "    )\n",
    "    alias = parent_alias.lower() if (mono_info.functionality == 1) else parent_alias.upper()\n",
    "    mono_infos[alias] = mono_info\n",
    "mono_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining polymer composition class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, StrEnum, auto\n",
    "\n",
    "from polymerist.genutils.fileutils.jsonio.serialize import JSONSerializable, TypeSerializer, MultiTypeSerializer\n",
    "from polymerist.genutils.fileutils.jsonio.jsonify import make_jsonifiable, JSONifiable\n",
    "from polymerist.polymers.monographs import MonomerGraph, MonomerGraphSerializer\n",
    "from polymerist.rdutils.bonding.portlib import get_num_linkers, get_ports\n",
    "\n",
    "\n",
    "MONOMER_CHARS = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
    "class MonomerNeighborMismatch(Enum):\n",
    "    '''For annotating the various ways in which a piece of monomer information in a monomer alphabet does not match a monomer graph'''\n",
    "    NONE = 0\n",
    "    COUNT = auto()\n",
    "    BONDTYPE = auto()\n",
    "    NO_FLAVOR = auto()\n",
    "    DIFF_FLAVOR = auto()\n",
    "    NOT_NEIGHBORS = auto()\n",
    "\n",
    "\n",
    "@make_jsonifiable(type_serializer=MultiTypeSerializer(MonomerGraphSerializer, MonomerFragmentInfo.serializer))\n",
    "@dataclass\n",
    "class PolymerStructure:\n",
    "    '''Encodes a multi-scale structural representation of a polymer topology'''\n",
    "    mono_alphabet : dict[str, MonomerFragmentInfo] \n",
    "    monograph : MonomerGraph\n",
    "\n",
    "    single_char_mononames : dict[str, str] = field(default_factory=dict, init=False) \n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        '''Post-process init attributes'''\n",
    "        self.single_char_mononames = { # remapping from the assigned monomers names to single characters for mbuild compatibility\n",
    "            mononame : remap_char\n",
    "                for (mononame, remap_char) in zip(self.mono_alphabet.keys(), MONOMER_CHARS)\n",
    "        }\n",
    "\n",
    "        self.validate_monoinfo_is_compatible() # will raise targetted exceptions if incompatible\n",
    "        self.assign_monoinfo_to_monograph()\n",
    "\n",
    "    @property\n",
    "    def node_info_map(self) -> dict[int, MonomerFragmentInfo]:\n",
    "        '''Map from node indices to relevant monomer information'''\n",
    "        return {\n",
    "            node_id : self.mono_alphabet[alias]\n",
    "                for node_id, alias in nx.get_node_attributes(self.monograph, self.monograph.MONOMER_NAME_ATTR).items()\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def pdb_substructures(self) -> dict[str, list[str]]:\n",
    "        '''Substructure dict formatted for the OpenFF Topology.from_pdb RDKit wrapper hook'''\n",
    "        return {\n",
    "            monoinfo.name : [mono_info.smarts]\n",
    "                for monoinfo in self.mono_alphabet.values()\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def num_atoms(self) -> int:\n",
    "        '''Total number of atoms in the topology specified'''\n",
    "        return sum(nx.get_node_attributes(self.monograph, 'contribution').values())\n",
    "\n",
    "    # validation\n",
    "    def monoalpha_surjective_to_monograph(self) -> bool:\n",
    "        '''Check whether the monomer alphabet covers all monomer types defined in the Graph'''\n",
    "        return self.monograph.unique_monomer_names.issubset(set(self.mono_alphabet.keys()))\n",
    "\n",
    "    def monoalpha_neighbors_are_valid(self) -> tuple[bool, int, MonomerNeighborMismatch]:\n",
    "        '''Determine whether and why adjacent monomers in the monomer graph do (or don't) have compatible chemical info'''\n",
    "        for node_idx, neighbor_dict in self.monograph.adj.items():\n",
    "            # 1) check that the number of neighbors in the graph matches the number of intermonomer bonding sites given chemically\n",
    "            neighbor_dict = dict(neighbor_dict) # convert from networkx object to vanilla dict\n",
    "            degree = len(neighbor_dict) # self.monograph.degree[node_idx]\n",
    "            if (degree != self.node_info_map[node_idx].functionality):\n",
    "                return False, node_idx, MonomerNeighborMismatch.COUNT\n",
    "            \n",
    "            # 2) check that all reported neighbor nodes are actually adjacent in the graph\n",
    "            found_ports = set()\n",
    "            for i, flavor in self.monograph.get_flavor_dict(node_idx).items():\n",
    "                if i not in neighbor_dict:\n",
    "                    return False, node_idx, MonomerNeighborMismatch.NOT_NEIGHBORS\n",
    "                \n",
    "                nb_bond_info = neighbor_dict.pop(i)\n",
    "                found_ports.add( (flavor, nb_bond_info[self.monograph.BONDTYPE_ATTR]) )\n",
    "\n",
    "            # 3) check that every neighbors has been provided a flavor\n",
    "            if neighbor_dict:\n",
    "                return False, node_idx, MonomerNeighborMismatch.NO_FLAVOR # at least one of the neighbors must nnot have had a flavor provided\n",
    "\n",
    "            # 4) check that the provided flavors match those chemically specified\n",
    "            monoinfo = self.mono_alphabet[self.monograph.get_monomer_name(node_idx)]\n",
    "            portinfo = set(\n",
    "                (port.flavor, port.bond.GetBondType())\n",
    "                    for port in get_ports(monoinfo.rdmol)\n",
    "            )\n",
    "            if (portinfo != found_ports):\n",
    "                return False, node_idx, MonomerNeighborMismatch.DIFF_FLAVOR\n",
    "        else:\n",
    "            return True, -1, MonomerNeighborMismatch.NONE\n",
    "        \n",
    "    def validate_monoinfo_is_compatible(self) -> None:\n",
    "        if not self.monoalpha_surjective_to_monograph():\n",
    "            raise ValueError('Provided monomer alphabet does not cover all monomers in the corresponding monomer graph')\n",
    "        \n",
    "        nb_match, mismatch_idx, reason = self.monoalpha_neighbors_are_valid()\n",
    "        if not nb_match:\n",
    "            raise ValueError(f'Graph node {mismatch_idx} (designation \"{self.monograph.get_monomer_name(mismatch_idx)}\") mismatched (reason : {reason.name})')\n",
    "    \n",
    "    def assign_monoinfo_to_monograph(self) -> None:\n",
    "        '''Map the chemical info for each unique monomer onto corresponding monomer nodes in the monomer graph'''\n",
    "        node_info_map = {\n",
    "            node_idx : self.mono_alphabet[self.monograph.monomer_name(node_idx)].__dict__\n",
    "                for node_idx in self.monograph.nodes\n",
    "        }\n",
    "        nx.set_node_attributes(self.monograph, node_info_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from polymerist.genutils.textual import delimiters\n",
    "from polymerist.polymers.monographs import MonomerGraph\n",
    "\n",
    "smidge = 'aBABABABa'\n",
    "# smidge = '{2-3}'.join(smidge)\n",
    "# smidge = '{0-1}'.join(smidge)\n",
    "smidge = '<0-1>'.join(smidge[:-1]) + '<0-0>' + smidge[-1]\n",
    "smidge = delimiters.square_brackets_around_letters(smidge)\n",
    "smidge = ''.join(3*c if c.isalpha() else c for c in smidge)\n",
    "print(smidge)\n",
    "\n",
    "monograph = MonomerGraph.from_smidge(smidge)\n",
    "monograph.draw()\n",
    "\n",
    "poly = PolymerStructure(\n",
    "    mono_alphabet=mono_infos,\n",
    "    monograph=monograph\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing JSON I/O\n",
    "poly.to_file('test.json')\n",
    "poly2 = PolymerStructure.from_file('test.json')\n",
    "poly2.monograph.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbuild.lib.recipes import Polymer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing mbuild coordinate generator hook for linear polymer graphs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly.monograph.is_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in poly.monograph.termini:\n",
    "    print(poly.monograph.nodes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polymerist.genutils.textual.strsearch import shortest_repeating_substring\n",
    "\n",
    "\n",
    "terms = list(monograph.termini)\n",
    "assert(len(terms) == 2)\n",
    "head_node = terms[0]\n",
    "\n",
    "seq = ''\n",
    "for i in nx.dfs_preorder_nodes(monograph, source=head_node):\n",
    "    if i not in terms:\n",
    "        mononame = monograph.nodes[i][monograph.MONOMER_NAME_ATTR]\n",
    "        seq += poly.single_char_mononames[mononame]\n",
    "min_seq = shortest_repeating_substring(seq)\n",
    "seq, min_seq, seq.count(min_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polymerist.rdutils.bonding.portlib import Port\n",
    "from polymerist.rdutils.bonding.substitution import saturate_ports\n",
    "\n",
    "Port.bondable_flavors.reset()\n",
    "Port.bondable_flavors.insert((0,2))\n",
    "Port.bondable_flavors.insert((1,2))\n",
    "\n",
    "rm = mono_info.rdmol\n",
    "newmol = saturate_ports(rm, cap=Chem.MolFromSmiles('*-[2H]', sanitize=False), flavor_to_saturate=0)\n",
    "display(newmol)\n",
    "for atom in newmol.GetAtoms():\n",
    "    print(atom.GetIdx(), atom.GetSymbol(), atom.GetIsotope())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polymerist.rdutils.bonding.substitution import hydrogenate_rdmol_ports\n",
    "from mbuild.conversion import from_rdkit\n",
    "from polymerist.polymers.building import mbmol_to_openmm_pdb\n",
    "\n",
    "\n",
    "prot_mol = hydrogenate_rdmol_ports(mono_info.rdmol)\n",
    "Chem.SanitizeMol(prot_mol, sanitizeOps=specification.SANITIZE_AS_KEKULE)\n",
    "mbmol = from_rdkit(prot_mol)\n",
    "mbmol_to_openmm_pdb('test.pdb', mbmol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with rich progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.progress import track, Progress\n",
    "from time import sleep\n",
    "\n",
    "with Progress() as progress:\n",
    "\n",
    "    task1 = progress.add_task(\"[red]Downloading...\", total=1000)\n",
    "    task2 = progress.add_task(\"[green]Processing...\", total=1000)\n",
    "    task3 = progress.add_task(\"[cyan]Cooking...\", total=1000)\n",
    "\n",
    "    while not progress.finished:\n",
    "        progress.update(task1, advance=0.5)\n",
    "        progress.update(task2, advance=0.3)\n",
    "        progress.update(task3, advance=0.9)\n",
    "        sleep(0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from rich.progress import (\n",
    "    BarColumn,\n",
    "    Progress,\n",
    "    SpinnerColumn,\n",
    "    TaskProgressColumn,\n",
    "    TimeElapsedColumn,\n",
    "    TimeRemainingColumn,\n",
    ")\n",
    "\n",
    "def process(chunks):\n",
    "    for chunk in chunks:\n",
    "        time.sleep(0.1)\n",
    "        yield chunk\n",
    "\n",
    "chunks = [random.randint(1,20) for _ in range(100)]\n",
    "\n",
    "progress_columns = (\n",
    "    SpinnerColumn(),\n",
    "    \"[progress.description]{task.description}\",\n",
    "    BarColumn(),\n",
    "    TaskProgressColumn(),\n",
    "    \"Elapsed:\",\n",
    "    TimeElapsedColumn(),\n",
    "    \"Remaining:\",\n",
    "    TimeRemainingColumn(),\n",
    ")\n",
    "\n",
    "with Progress(*progress_columns) as progress_bar:\n",
    "    task = progress_bar.add_task(\"[blue]Downloading...\", total=sum(chunks))\n",
    "    for chunk in process(chunks):\n",
    "        progress_bar.update(task, advance=chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "from rich.live import Live\n",
    "from rich.table import Table\n",
    "\n",
    "\n",
    "def generate_table() -> Table:\n",
    "    \"\"\"Make a new table.\"\"\"\n",
    "    table = Table()\n",
    "    table.add_column(\"ID\")\n",
    "    table.add_column(\"Value\")\n",
    "    table.add_column(\"Status\")\n",
    "\n",
    "    for row in range(random.randint(2, 6)):\n",
    "        value = random.random() * 100\n",
    "        table.add_row(\n",
    "            f\"{row}\", f\"{value:3.2f}\", \"[red]ERROR\" if value < 50 else \"[green]SUCCESS\"\n",
    "        )\n",
    "    return table\n",
    "\n",
    "\n",
    "with Live(generate_table(), refresh_per_second=4) as live:\n",
    "    for _ in range(40):\n",
    "        time.sleep(0.4)\n",
    "        live.update(generate_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from rich.console import Console, ConsoleOptions, RenderResult\n",
    "from rich.table import Table\n",
    "\n",
    "@dataclass\n",
    "class Student:\n",
    "    id: int\n",
    "    name: str\n",
    "    age: int\n",
    "    def __rich_console__(self, console: Console, options: ConsoleOptions) -> RenderResult:\n",
    "        yield f\"[b]Student:[/b] #{self.id}\"\n",
    "        my_table = Table(\"Attribute\", \"Value\")\n",
    "        my_table.add_row(\"name\", self.name)\n",
    "        my_table.add_row(\"age\", str(self.age))\n",
    "        yield my_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polymerist-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
